<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>How to train your Lipschitz Network | ls ~niklas/</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="How to train your Lipschitz Network" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="RS @Anthropic, previously @Meta AI (FAIR), @MIT and @CERN" />
<meta property="og:description" content="RS @Anthropic, previously @Meta AI (FAIR), @MIT and @CERN" />
<link rel="canonical" href="https://nolte.dev/jekyll/update/2022/02/13/training-lipschitz-networks.html" />
<meta property="og:url" content="https://nolte.dev/jekyll/update/2022/02/13/training-lipschitz-networks.html" />
<meta property="og:site_name" content="ls ~niklas/" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-13T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="How to train your Lipschitz Network" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-02-13T00:00:00+00:00","datePublished":"2022-02-13T00:00:00+00:00","description":"RS @Anthropic, previously @Meta AI (FAIR), @MIT and @CERN","headline":"How to train your Lipschitz Network","mainEntityOfPage":{"@type":"WebPage","@id":"https://nolte.dev/jekyll/update/2022/02/13/training-lipschitz-networks.html"},"url":"https://nolte.dev/jekyll/update/2022/02/13/training-lipschitz-networks.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://nolte.dev/feed.xml" title="ls ~niklas/" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">ls ~niklas/</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About me</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">How to train your Lipschitz Network</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-02-13T00:00:00+00:00" itemprop="datePublished">Feb 13, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>

<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h2 id="what-are-lipschitz-networks">What are Lipschitz Networks?</h2>
<p>In my endeavours to try to make AI more safe and understandable, I make use of
neural networks which I call “Lipschitz Networks” (even though this term is not
consistently used in literature).</p>

<p>Lipschitz networks constrain the gradient p-norm $|\nabla_x f(x)|_p$ of your
network with respect to the inputs to a maximum of your choice, lets say 1.  In
practice, there are multiple ways to do this.
The requirements for a suitable implementation are as follows:</p>
<ol>
  <li>$|\nabla_x f(x)|_p \leq 1$.</li>
  <li>Can approximate every $f(x)$ to arbitrary precision if 1. is fulfilled</li>
</ol>

<h3 id="the-layerwise-constraint">The Layerwise Constraint</h3>
<p>One very safe or deterministic way of implementing requirement 1 is constraining the
jacobian operator norm of each layer with respect to the input.  In fully
connected networks, the weight matrices coincide with the jacobian, so it is
convenient to constrain these directly, layerwise: $|W^i|_p \leq 1 \ \forall i$</p>

<p>However, a layerwise constraints can overdo the trick.  Since the Lipschitz
constant of a (fully connected) neural network is determined by the product of
the jacobians and the lipschitz constants of the activations, a layerwise
constraint easily accumulates into something that is much smaller than 1 and
cannot recover the “full gradient”. <a class="citation" href="#anil2019sorting">(Anil et al., 2019)</a> refer to this as
gradient norm attenuation.</p>

<h3 id="the-groupsort-activation">The GroupSort activation</h3>
<p>The specific problem is the fact that the usual activation functions, while
being Lipschitz-1, cannot maintain a maximum allowed gradient everywhere.
For instance, if one neuron has a preactivation of $&lt; 0$, ReLU will result in
a gradient of 0 there and a possible $| \nabla_x f(x) | = 1$ is unachievable.
<a class="citation" href="#anil2019sorting">(Anil et al., 2019)</a> show this very nicely by trying to fit a layerwise constrained
network with ReLU activation to the absolute value function. Spoiler: It does not work.</p>

<p>So they went ahead and derived a new activation function: <em>GroupSort</em>.  It sorts
the preactivations within n subgroups of the input. Example: GroupSort(1) is the
full sort operation, GroupSort(d/2) is the MaxMin operation.
Since it is merely a permutation, it maintains gradient 1 everywhere, while being
a sufficient nonlinearity to serve as activation. Together with a specific constraint,
they are able to prove universal approximation of GroupSort Lipschitz networks!<br />
The weight norm constraint to achieve p-normed Lipschitzness is:</p>

\[\begin{align}
|W^1|_{p,\infty} &amp;\leq 1 \\
|W^i|_\infty &amp;\leq 1 \ \ \forall \ i &gt; 1
\end{align}\]

<h3 id="training-the-lipschitz-network">Training the Lipschitz Network</h3>
<p>Ok, so let us train a Lipschitz network for some binary classification task!
For training data, let’s use the two-moons dataset.  Using BCE as loss and Adam
as optimizer, we can train a Lipschitz network with a Lipschitz constant of 1.
We immediately see that the network is unable to achieve a good classification
performance.</p>

<p><img src="/files/images/two-moons-lipschitz-bce.png" alt="bce" width="50%" /></p>

<p>The reason for this is not that the gradient is too constrained.  In fact, we
should be able to achieve perfect classification performance with any Lipschitz
constant &gt; 0, because the decision frontier is defined only by the sign of the
output (or the sign of output - 0.5 if it’s in [0,1]), and the sign is
scale invariant. So why does this not work?</p>

<p>Recall how BCE works: It tries to maximize the margins, i.e. get the output for
class 0 as close as possible to 0 and the output of class 1 as close as possible
to 1. With a sigmoid as output activation, this means unbounded increase for
the preactivations to minimize BCE.<br />
In unconstrained networks, that is fine and actually desirable.  In a Lipschitz
network, it is unadvisable to concentrate on margin maximization because, when
the gradient is bounded, the objective may clash with the actual goal of
classification: Maximizing accuracy.  A loss function that cares about margin
maximization up until a certain point is much better suited here: Hinge loss!</p>

<p><img src="/files/images/two-moons-lipschitz-hinge.png" alt="hinge" width="49%" /></p>

<p>Hinge loss does not assign a penalty to training data outside of a margin of
specified size, so the Lipschitz network can concentrate its efforts on
optimizing the decision frontier.</p>

<p>More on this when I find more time.</p>

<h3 id="references">References</h3>
<ol class="bibliography"><li><span id="anil2019sorting">Anil, C., Lucas, J., &amp; Grosse, R. (2019). <i>Sorting out Lipschitz function approximation</i>.</span></li></ol>

  </div><a class="u-url" href="/jekyll/update/2022/02/13/training-lipschitz-networks.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">ls ~niklas/</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">ls ~niklas/</li><li><a class="u-email" href="mailto:niklas'at'nolte'dot'dev">niklas'at'nolte'dot'dev</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/niklasnolte"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">niklasnolte</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>RS @Anthropic, previously @Meta AI (FAIR), @MIT and @CERN</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
